
\section{Penalized linear regression}
\label{sec:linreg}
In this section we look at a linear regression model which we use to predict the
$p_c$ and $r_c$ for all configurations. Suppose we have a datasets $(Y_0, Z_0),
(Y_1, X_1),\cdots, (Y_n, X_n), $ where $Y_i \in \R^k$, $X_i \in \R^{n\times p}$
and $Z_i \in \R^{m\times p}$ ($m \ll n$). We want to fit a linear model in $Y_0$
and $Z_0$ and use it to predict $p_c $ and $r_c$ for all $c \in \C$, but we do not have sufficient data points in $Z_0$. On the other hand, the other datasets$  (Y_1, X_1),\cdots, (Y_n, X_n) $ have large number of samples. Also, as a prior knowledge we know that some of the datasets are quite similar to the dataset $(Y_0, Z_0)$, hence the slope coefficients should be very similar.
$$
Y_0 = Z_0 \beta_0 + \epsilon, \text{where } \epsilon \sim N(0,\sigma^2 \I)
$$
For other datasets we have the similar assumption,
$$
Y_i = X_i \beta_i + \epsilon, \text{where } \epsilon \sim N(0,\sigma^2 \I)
$$
Using all this information we frame the following regression optimization problem,

$$
\hat{\beta}_0 = \text{argmax}_{\hat{\beta}_0} \| Y_0 - Z_0 \hat{\beta}_0\|^2_2 + \sum_{i=1}^k \lambda_i  \| {\hat{\beta}_i} - {\hat{\beta}_0} \|^2_2
$$
Then, $\hat{\beta}_0 = (Z_0^TZ_0 + \sum_{i=1}^k \lambda_i \I)^{-1} (Z_0^TY_0 + \sum_{i=1}^k \lambda_i\hat{\beta}_i)$ and the bias and variance decomposition is given by,
$$
\text{Bias: }  (Z_0^TZ_0 + \sum_{i=1}^k \lambda_i \I)^{-1}  \sum_{i=1}^k \lambda_i(\beta_i- \beta_0)
$$
$$
\text{Variance}:
\sigma^2  X_0 
(Z_0^TZ_0 + \sum_{i=1}^k \lambda_i \I)^{-1} 
(Z_0^TZ_0 + \sum_{i=1}^k \lambda_i {(X_i^TX_i)}^{-1}) 
(Z_0^TZ_0 + \sum_{i=1}^k \lambda_i \I)^{-1}X_0^T
$$

%TODO Leave one out cross validation statistic
\begin{comment}
\begin{figure}[t!]\vspace*{-5pt}
\centering
  \begin{tabular}[h]{cc}\hspace*{-15pt}
  \includegraphics[width=0.5\textwidth]{Figures/pcf-acf.png}&
   \includegraphics[width=0.5\textwidth]{Figures/pcf-acfY.png}\\
  (a)&(b)\\\vspace*{-10pt}
  \end{tabular}
  \caption{
Plot of autocorrelation values for residuals of Performance of Kmeans32threads data. a) Residues b) Performance
  }
\label{fig:simulation}
\end{figure}

\begin{figure}[t!]\vspace*{-5pt}
\centering
  \begin{tabular}[h]{ccc}\hspace*{-15pt}
  \includegraphics[width=0.375\textwidth]{Figures/residuevscore.png}&
   \includegraphics[width=0.375\textwidth]{Figures/residuevsfrequency.png}&
  \includegraphics[width=0.375\textwidth]{Figures/residuevsmemory.png}\\
 \includegraphics[width=0.375\textwidth]{Figures/residuevscore_2.png}&
   \includegraphics[width=0.375\textwidth]{Figures/residuevsfrequency_2.png}&
  \includegraphics[width=0.375\textwidth]{Figures/residuevsmemory_2.png}\\
  (a)&(b)&(c)\\\vspace*{-10pt}
  \end{tabular}
  \caption{
Plot of residuals vs different X. a) Cores b) Frequency c) Memory.
  }
\label{fig:simulation}
\end{figure}
\end{comment}

